{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges & Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations encountered in Section 1: Downloading data\n",
    "- The limitations encountered while using Coinmarket API\n",
    "- The limitations encountered with python web-scraping\n",
    "\n",
    "\n",
    "\n",
    "### The limitations encountered while using Coinmarketcap API\n",
    "The instruction for this was \"*It's best if you connect to the API, but if this information is not available in their API you could use a python web scraping tool to do the job.*\"\n",
    "Coinmarketcap has a really good API which i signed up for but I am limited to their **basic** subscription. This restricts me from accessing API endpoint which has all the columns I need which is the **\"Market pairs\"** endpoint, this is stated clearly in their documentation [here](https://coinmarketcap.com/api/documentation/v1/#operation/getV1ExchangeMarketpairsLatest). \n",
    "> If you are interested in seeing my attempt and the data available on their **basic plan**, check **my google colab [Click here](https://colab.research.google.com/drive/1Xq3eQlm7mN41oIhZIsQrhEw578ymtQXu?usp=sharing)**\n",
    "\n",
    "Next, I tried getting their API through **RapidAPI API's Hub**, Even though I found one [Click here](https://rapidapi.com/community/api/coinbase/), unfortunately it does not have the data endpoint needed to populate my columns for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The limitations encountered with python web-scraping\n",
    "Python's Selenium and Beautifulsoup was used for webscraping. I used three methods to approach this, there was limitations to each method:\n",
    "1. using only beautifulsoup library\n",
    "2. using only selenium library\n",
    "3. using the combination of selenium and beautifulsoup library.\n",
    "\n",
    "Now, I will show some codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using only beautifulsoup library.\n",
    "\n",
    "---\n",
    "The limitation for this method was beautifulsoup could not scrape market table, resulting in a None result. \n",
    "\n",
    "**Reasons the scraping possibly failed;**\n",
    "- the page's table is dynamic, beautifulsoup cannot work with dynamic pages \n",
    "- the page has too much underlyting javascript, beautifulsoup cannot work in such pages\n",
    "\n",
    "Now, I will show some codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Assign bitcoin market link to a variable\n",
    "link = 'https://coinmarketcap.com/currencies/bitcoin/markets/'\n",
    "\n",
    "# use request to get the raw HTML content from the website\n",
    "HTML_script = requests.get(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The HTTP 200 OK success status response code indicates that the request has succeeded \n",
    "HTML_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now to extract the text from the HTML_script\n",
    "text_script = HTML_script.text\n",
    "\n",
    "# where 'html.parser' is a processing tool\n",
    "useful_text = BeautifulSoup(text_script, \"lxml\") \n",
    "\n",
    "# now print the useful_text using prettify module.\n",
    "# print(useful_text.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitcoin price today, BTC to USD live, marketcap and chart | CoinMarketCap\n"
     ]
    }
   ],
   "source": [
    "# printing the pages's title\n",
    "print(useful_text.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# after inspecting the page I obtained the market table's html\n",
    "# calling bs4 to find table by specifing the class\n",
    "table = useful_text.find('table', {'class':'h7vnx2-2 ecUULi cmc-table  '})\n",
    "\n",
    "# bs4 is unable to scrape the market table hence 'none' result is returned\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using only selenium library.\n",
    "\n",
    "---\n",
    "The python code was used appropriately, selenium could scrape market table but empty results were returned for some columns, resulting in too many empty values. This could be caused by any reasons unknown to me. I tried troubleshooting but;\n",
    "- Selenium is not well documented in python, making it hard to troubleshoot\n",
    "- Support for selenium using python support is not as wide as selenium using java, so its not at its full capacity on python\n",
    "\n",
    "Now, I will show some codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load selenium components\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "# Establish chrome driver and go to bitcoin market URL\n",
    "url = 'https://coinmarketcap.com/currencies/bitcoin/markets/'\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "# scraping the table body by xpath then scrappping all table rows by xpath\n",
    "rows = driver.find_element_by_xpath('//*[@id=\"__next\"]/div/div[1]/div[2]/div/div[3]/div/div[2]/div/table/tbody').find_elements_by_tag_name('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bitcoin Markets'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scraping the page title\n",
    "driver.find_element_by_xpath('//*[@id=\"__next\"]/div/div[1]/div[2]/div/div[3]/div/div[1]/div[1]/h2').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping 'sources' column of market table works quite fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "['Binance', 'Huobi Global', 'Coinbase Exchange', 'Binance', 'FTX', 'KuCoin', 'Binance', 'Bitfinex', 'Coincheck', 'Gate.io', 'Binance', 'bitFlyer', 'FTX', 'Kraken', 'Kraken', 'Binance', 'Bitstamp', 'Binance', 'Bithumb', 'FTX US', 'Liquid', 'Binance', 'Gemini', 'Binance', 'Coinbase Exchange', 'FTX US', 'Binance', 'Binance', 'Binance.US', 'Binance', 'Binance', 'Bitfinex', 'Coinbase Exchange', 'Bitstamp', 'Coinbase Exchange', 'Huobi Global', 'Binance', 'Binance', 'Binance', 'Poloniex', 'Binance', 'Coinbase Exchange', 'Coinbase Exchange', 'Binance', 'Binance', 'Huobi Global', 'Bitfinex', 'Binance', 'Gate.io', 'Binance', 'Binance', 'Binance', 'Binance', 'Coinbase Exchange', 'Bitfinex', 'FTX', 'Kraken', 'Binance', 'Binance', 'Binance', 'Binance', 'Bitfinex', 'KuCoin', 'bitFlyer', 'KuCoin', 'FTX', 'Binance', 'Binance', 'Binance', 'Binance', 'Binance', 'Binance', 'Binance', 'Binance', 'Coinbase Exchange', 'Binance.US', 'Coinbase Exchange', 'Binance', 'Kraken', 'Binance', 'Bitfinex', 'Binance', 'Binance', 'Poloniex', 'Huobi Global', 'Bittrex', 'Binance', 'Huobi Global', 'Huobi Global', 'Bittrex', 'Binance', 'bitFlyer', 'Huobi Global', 'Binance', 'Binance', 'Binance', 'Binance', 'FTX', 'KuCoin', 'Binance']\n"
     ]
    }
   ],
   "source": [
    "source=[]\n",
    "\n",
    "# scraping the first 100 rows\n",
    "for x in range(100):\n",
    "    #get name\n",
    "    source.append(rows[x].find_elements_by_tag_name('td')[1].text)\n",
    "    \n",
    "# printing the total number of rows with values\n",
    "print(len(source))\n",
    "\n",
    "# printing the data scraped\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However, whilst scraping 'pairs' column of the table, a bad pattern of too many empty results are noticed here - just 16 out of 100 returned values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BTC/USDT', 'BTC/USD', 'BTC/USD', 'BTC/USDT', 'BTC/BUSD', 'BTC/USDT', 'ETH/BTC', 'BTC/USDT', 'BTC/USD', 'BTC/USDT', 'BTC/JPY', 'XBT/USD', 'FTM/BTC', 'XBT/EUR', 'BTC/JPY', 'BTC/EUR', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs=[]\n",
    "\n",
    "# scraping the first 100 rows\n",
    "for x in range(100):\n",
    "\n",
    "    #get pairs\n",
    "    pairs.append(rows[x].find_elements_by_tag_name('td')[2].text)\n",
    "\n",
    "# printing the total number of rows with values\n",
    "len(pairs)\n",
    "\n",
    "# printing the data scraped\n",
    "print(pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same pattern of too many empty results are noticed in 'price' column - just  16 out of 100 have values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "['$54,587.58', '$54,601.31', '$54,599.35', '$54,589.55', '$54,602.00', '$54,636.28', '$54,636.31', '$54,632.00', '$54,670.95', '$54,637.74', '$54,636.31', '$54,620.43', '$54,618.58', '$54,611.70', '$54,639.97', '$54,209.70']\n"
     ]
    }
   ],
   "source": [
    "price=[]\n",
    "\n",
    "for row in rows[0:]:\n",
    "    try:\n",
    "        \n",
    "        price.append(row.find_elements_by_tag_name('td')[3].text)\n",
    "    except AttributeError:\n",
    "        break\n",
    "    except IndexError:\n",
    "        break\n",
    "# printing the total number of rows with values              \n",
    "print(len(price))\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using the combination of selenium and beautifulsoup library.\n",
    "Limited data was still obtained with this method.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1 : Download data\n",
    "\n",
    "#### Webscraping is not a sure-fire method, APIs are more reliable. For a website with busy traffic like coinmarketcap that embeds other websites data and updates them in real-time, any of their underlying javascript could be interferring with the webscraping process. As a result, the data obtained from the website would be limited. However, coinmarket API paid subscriptions(professional and enterprise) could be more efficient in getting more data with ease.\n",
    "\n",
    "#### Please keep in mind, for the sake of seeing this project to completion combination of selenium and beautifulsoup libraries will be used, the downside to this method is limited data scraping.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Visualize\n",
    "\n",
    "Python's library pandas was utilized for the required tasks\n",
    "\n",
    "- There was no limitations found in this aspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Plot\n",
    "Python's library malplotlib was utilized for the required tasks\n",
    "\n",
    "- There was no limitations found in this aspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, move to Section 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
